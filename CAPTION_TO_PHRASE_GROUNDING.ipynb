{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's download the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers flash_attn timm einops peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import cv2\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import html\n",
    "import base64\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "from google.colab import userdata\n",
    "from IPython.core.display import display, HTML\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    get_scheduler\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Generator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
    "REVISION = 'refs/pr/6'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION).to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "output_folder = \"processed_frames\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Frame and Find Details Caption from the Image and Use that Caption for Caption-to-Phrase Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a folder to store the processed images\n",
    "output_folder = \"processed_frames2\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "task = \"<DETAILED_CAPTION>\"\n",
    "text = \"<DETAILED_CAPTION>\"\n",
    "\n",
    "cap = cv2.VideoCapture('/content/input_video2.mp4')\n",
    "\n",
    "frame_number = 1\n",
    "frame_and_caption = {}  # Initialize dictionary to store frame numbers and captions\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # Exit the loop if no more frames are available\n",
    "\n",
    "\n",
    "    \n",
    "    frame_count += 1\n",
    "    if frame_count % 10 == 0:\n",
    "      \n",
    "      \n",
    "      \n",
    "      inputs = processor(text=text, images=frame, return_tensors=\"pt\").to(DEVICE)\n",
    "      generated_ids = model.generate(\n",
    "              input_ids=inputs[\"input_ids\"],\n",
    "              pixel_values=inputs[\"pixel_values\"],\n",
    "              max_new_tokens=1024,\n",
    "              num_beams=3\n",
    "          )\n",
    "\n",
    "      generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "      response = processor.post_process_generation(generated_text, task=task, image_size=(frame.shape[1], frame.shape[0]))\n",
    "\n",
    "          # Store the generated caption for the current frame\n",
    "      if '<DETAILED_CAPTION>' in response:\n",
    "              frame_and_caption[frame_number] = response['<DETAILED_CAPTION>']\n",
    "\n",
    "\n",
    "      task_2 = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "      text_2 = f\"<CAPTION_TO_PHRASE_GROUNDING> {response['<DETAILED_CAPTION>']}\"\n",
    "\n",
    "      inputs = processor(text=text_2, images=frame, return_tensors=\"pt\").to(DEVICE)\n",
    "      generated_ids_2= model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            num_beams=3\n",
    "        )\n",
    "      generated_text_2 = processor.batch_decode(generated_ids_2, skip_special_tokens=False)[0]\n",
    "      response_2 = processor.post_process_generation(generated_text_2, task=task_2, image_size=(frame.shape[1], frame.shape[0]))\n",
    "\n",
    "      bboxes = response_2['<CAPTION_TO_PHRASE_GROUNDING>']['bboxes']\n",
    "      labels = response_2['<CAPTION_TO_PHRASE_GROUNDING>']['labels']\n",
    "\n",
    "      for bbox, label in zip(bboxes, labels):\n",
    "                # Extract bounding box coordinates\n",
    "              x1, y1, x2, y2 = [int(coord) for coord in bbox]\n",
    "\n",
    "                # Draw the rectangle for the bounding box\n",
    "              cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                # Put the label text above the bounding box\n",
    "              cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "      processed_frame = frame  # Store the frame in a variable\n",
    "      output_filename = f\"{output_folder}/frame_{frame_number}.jpg\"\n",
    "      cv2.imwrite(output_filename, processed_frame)  # Save the processed image to the folder\n",
    "\n",
    "      frame_number += 1\n",
    "\n",
    "\n",
    "    # Break the loop on pressing 'Esc'\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the stored frame and caption data\n",
    "print(frame_and_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download output image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to the folder and zip file\n",
    "folder_path = '/content/processed_frames2'\n",
    "zip_path = '/content/processed_frames2.zip'\n",
    "\n",
    "# Remove the existing zip file if it exists\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)\n",
    "\n",
    "# Create a zip file of the folder\n",
    "shutil.make_archive('/content/processed_frames2', 'zip', folder_path)\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(zip_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
